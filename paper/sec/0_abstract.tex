\begin{abstract}
%We present a fast, scalable video classification framework that leverages pretrained multimodal embeddings and shallow neural networks. By combining visual, audio, and text metadata embeddings in a supervised pipeline, we achieve significant accuracy gains over zero-shot video embedding classifiers and fine-tuned TimeSformer model while using an order of magnitude less compute. Ablation studies on a balanced 10-class subset of YouTube-8M demonstrate the relative contributions of each modality and the robustness of embedding-space augmentation. Our results show that pretrained multimodal features provide semantically rich representations that enable efficient, flexible video classification.

This paper presents a fast and scalable video classification framework that leverages pre-trained multimodal embedding and shallow neural networks.
The integration of visual, audio, and text metadata embeddings within a supervised pipeline has been demonstrated to yield substantial enhancement in accuracy compared to zero shot video embedding classifiers and fine-tuned TimeSformer model, while exhibiting a reduction in computational demand of up to an order of magnitude.
% Ablation studies on a balanced 10-class subset of YouTube-8M demonstrate the relative contributions of each modality and the robustness of embedding-space augmentation.
Our ablation studies demonstrate the relative contributions of each modality and the robustness of embedding-space augmentation.
The findings of this study demonstrate that pre-trained multimodal features yield semantically rich representations, thus facilitating efficient and flexible video classification.

\end{abstract}